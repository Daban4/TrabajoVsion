{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8eeb72a-d367-4376-9e3c-5c092fd7207e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rembg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrembg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rembg'"
     ]
    }
   ],
   "source": [
    "from rembg import remove\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Cargar imagen y eliminar fondo\n",
    "with open('cat_dog_100/train/dog/dog.10095.jpg', 'rb') as f:\n",
    "    input_image = f.read()\n",
    "\n",
    "output_image = remove(input_image)\n",
    "\n",
    "# Guardar el resultado sin fondo\n",
    "with open('perro_sin_fondo.png', 'wb') as f:\n",
    "    f.write(output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2723050d-c0da-45a5-bb6f-31c0c51f9a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[191 186 156 ... 124 149 166]\n",
      " [175 169 153 ... 135 146 154]\n",
      " [156 148 154 ... 134 130 129]\n",
      " ...\n",
      " [188 171 155 ... 187 169 132]\n",
      " [116 133 156 ... 192 186 157]\n",
      " [116 138 161 ... 212 223 215]]\n",
      "[[255 255 156 ... 124 149 255]\n",
      " [255 255 153 ... 135 146 154]\n",
      " [156 148 154 ... 134 130 129]\n",
      " ...\n",
      " [255 255 155 ... 255 255 132]\n",
      " [116 133 156 ... 255 255 157]\n",
      " [116 138 255 ... 255 255 255]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "imagen = cv2.imread('cat_dog_100/train/dog/dog.10089.jpg', 0)\n",
    "h, bins = np.histogram (imagen, bins = 256, range = [0, 256])\n",
    "q = np.median(imagen)\n",
    "\n",
    "print(imagen)\n",
    "segmentada = imagen\n",
    "segmentada[imagen>q] = 255\n",
    "print(segmentada)\n",
    "\n",
    "          \n",
    "cv2.imshow('dst',segmentada)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6af7eba7-73a1-4f74-b405-8e3ce358a476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo características HOG...\n",
      "Entrenando modelo SVM...\n",
      "Evaluando modelo...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.59      0.67       100\n",
      "           1       0.67      0.83      0.74       100\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.72      0.71      0.71       200\n",
      "weighted avg       0.72      0.71      0.71       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Preprocesamiento\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:  # Manejar imágenes no cargadas\n",
    "        print(f\"Error al cargar la imagen: {image_path}\")\n",
    "        return None\n",
    "    image = cv2.resize(image, (128, 128))  # Redimensionar a 128x128\n",
    "    image = cv2.equalizeHist(image)        # Ecualización del histograma\n",
    "    return image\n",
    "\n",
    "# 2. Extracción de características HOG\n",
    "def extract_hog_features(image):\n",
    "\n",
    "    fd = hog(\n",
    "        image,\n",
    "        orientations=8,\n",
    "        pixels_per_cell=(16, 16),\n",
    "        cells_per_block=(1, 1),\n",
    "        visualize=False  # Solo devuelve el descriptor\n",
    "    )\n",
    "    return fd\n",
    "\n",
    "# 3. Creación del dataset\n",
    "def create_dataset(image_dir, feature_extractor):\n",
    "    \"\"\"\n",
    "    Crea un dataset de características y etiquetas a partir de un directorio.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for label, class_name in enumerate(['cat', 'dog']):\n",
    "        class_dir = os.path.join(image_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, file_name)\n",
    "            image = preprocess_image(image_path)\n",
    "            if image is None:  # Omitir imágenes no válidas\n",
    "                continue\n",
    "            features = feature_extractor(image)\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def main_pipeline():\n",
    "    # Definir rutas de datos\n",
    "    train_dir = 'cat_dog_500/train'\n",
    "    test_dir = 'cat_dog_500/test'\n",
    "\n",
    "    # Crear datasets usando HOG\n",
    "    print(\"Extrayendo características HOG...\")\n",
    "    X_train, y_train = create_dataset(train_dir, extract_hog_features)\n",
    "    X_test, y_test = create_dataset(test_dir, extract_hog_features)\n",
    "\n",
    "    # Normalización (opcional si las características no están en el mismo rango)\n",
    "    X_train = (X_train - np.min(X_train)) / (np.max(X_train) - np.min(X_train))\n",
    "    X_test = (X_test - np.min(X_test)) / (np.max(X_test) - np.min(X_test))\n",
    "\n",
    "    # Entrenamiento del modelo\n",
    "    print(\"Entrenando modelo SVM...\")\n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluación del modelo\n",
    "    print(\"Evaluando modelo...\")\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Ejecutar el pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    main_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0bf47d1-6e58-402d-8a49-1a2f0dbc78d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo características HOG...\n",
      "Entrenando modelo SVM...\n",
      "Evaluando modelo...\n",
      "Reporte de Clasificación :\n",
      "\n",
      "Clase gatos:\n",
      "  precision: 0.78\n",
      "\n",
      "Clase perros:\n",
      "  precision: 0.67\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Preprocesamiento\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:  # Manejar imágenes no cargadas\n",
    "        print(f\"Error al cargar la imagen: {image_path}\")\n",
    "        return None\n",
    "    image = cv2.resize(image, (128, 128))  # Redimensionar a 128x128\n",
    "    image = cv2.equalizeHist(image)        # Ecualización del histograma\n",
    "    return image\n",
    "\n",
    "# 2. Extracción de características HOG\n",
    "def extract_hog_features(image):\n",
    "\n",
    "    fd = hog(\n",
    "        image,\n",
    "        orientations=8,\n",
    "        pixels_per_cell=(16, 16),\n",
    "        cells_per_block=(1, 1),\n",
    "        visualize=False  # Solo devuelve el descriptor\n",
    "    )\n",
    "    return fd\n",
    "\n",
    "# 3. Creación del dataset\n",
    "def create_dataset(image_dir, feature_extractor):\n",
    "    \"\"\"\n",
    "    Crea un dataset de características y etiquetas a partir de un directorio.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for label, class_name in enumerate(['cat', 'dog']):\n",
    "        class_dir = os.path.join(image_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, file_name)\n",
    "            image = preprocess_image(image_path)\n",
    "            if image is None:  # Omitir imágenes no válidas\n",
    "                continue\n",
    "            features = feature_extractor(image)\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def clasificacion(y_true, y_pred):\n",
    "    # Definir nombres de clases\n",
    "    class_names = {0: \"gatos\", 1: \"perros\"}\n",
    "\n",
    "    # Inicializar un diccionario para almacenar las métricas por clase\n",
    "    report = {}\n",
    "    for cls in [0, 1]:\n",
    "        # Calcular las métricas por clase\n",
    "        precision = precision_score(y_true, y_pred, labels=[cls], average=\"micro\")\n",
    "        report[class_names[cls]] = {\n",
    "            'precision': precision\n",
    "        }\n",
    "    # Imprimir el reporte de clasificación manual\n",
    "    print(\"Reporte de Clasificación :\")\n",
    "    for class_name, metrics in report.items():\n",
    "        print(f\"\\nClase {class_name}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value:.2f}\")\n",
    "\n",
    "# Pipeline actualizado\n",
    "def main_pipeline():\n",
    "    train_dir = 'cat_dog_500/train'\n",
    "    test_dir = 'cat_dog_500/test'\n",
    "\n",
    "    print(\"Extrayendo características HOG...\")\n",
    "    X_train, y_train = create_dataset(train_dir, extract_hog_features)\n",
    "    X_test, y_test = create_dataset(test_dir, extract_hog_features)\n",
    "\n",
    "    print(\"Entrenando modelo SVM...\")\n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Evaluando modelo...\")\n",
    "    y_pred = clf.predict(X_test)\n",
    "    clasificacion(y_test, y_pred)\n",
    "\n",
    "\n",
    "main_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd66192b-0904-4d08-a56b-e86dde710166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo características HOG...\n",
      "Normalizando características...\n",
      "Realizando búsqueda de hiperparámetros con GridSearchCV...\n",
      "Mejores parámetros encontrados: {'C': 1, 'gamma': 'scale'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       gatos       0.99      0.77      0.87       100\n",
      "      perros       0.81      0.99      0.89       100\n",
      "\n",
      "    accuracy                           0.88       200\n",
      "   macro avg       0.90      0.88      0.88       200\n",
      "weighted avg       0.90      0.88      0.88       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Preprocesamiento\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:  # Manejar imágenes no cargadas\n",
    "        print(f\"Error al cargar la imagen: {image_path}\")\n",
    "        return None\n",
    "    image = cv2.resize(image, (128, 128))  # Redimensionar a 128x128\n",
    "    image = cv2.equalizeHist(image)        # Ecualización del histograma\n",
    "    return image\n",
    "\n",
    "# 2. Extracción de características HOG\n",
    "def extract_hog_features(image):\n",
    "    fd = hog(\n",
    "        image,\n",
    "        orientations=9,  # Aumentamos el número de orientaciones\n",
    "        pixels_per_cell=(8, 8),  # Reducimos el tamaño de las celdas para capturar más detalles\n",
    "        cells_per_block=(2, 2),  # Aumentamos el tamaño del bloque para mejorar la normalización\n",
    "        visualize=False  # Solo devuelve el descriptor\n",
    "    )\n",
    "    return fd\n",
    "\n",
    "# 3. Creación del dataset\n",
    "def create_dataset(image_dir, feature_extractor):\n",
    "    \"\"\"\n",
    "    Crea un dataset de características y etiquetas a partir de un directorio.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for label, class_name in enumerate(['cat', 'dog']):\n",
    "        class_dir = os.path.join(image_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, file_name)\n",
    "            image = preprocess_image(image_path)\n",
    "            if image is None:  # Omitir imágenes no válidas\n",
    "                continue\n",
    "            features = feature_extractor(image)\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 4. Clasificación\n",
    "def clasificacion(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"gatos\", \"perros\"]))\n",
    "\n",
    "# Pipeline actualizado con búsqueda de hiperparámetros (GridSearchCV)\n",
    "def main_pipeline():\n",
    "    train_dir = 'cat_dog_500/train'\n",
    "    test_dir = 'cat_dog_500/test'\n",
    "\n",
    "    print(\"Extrayendo características HOG...\")\n",
    "    X_train, y_train = create_dataset(train_dir, extract_hog_features)\n",
    "    X_test, y_test = create_dataset(test_dir, extract_hog_features)\n",
    "\n",
    "    print(\"Normalizando características...\")\n",
    "    # Normalizamos las características\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Realizando búsqueda de hiperparámetros con GridSearchCV...\")\n",
    "    parameters = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}\n",
    "    clf = SVC(kernel='rbf')\n",
    "    grid_search = GridSearchCV(clf, parameters, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Obtener los mejores parámetros\n",
    "    print(\"Mejores parámetros encontrados:\", grid_search.best_params_)\n",
    "\n",
    "    # Evaluando el modelo con los mejores parámetros\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    clasificacion(y_test, y_pred)\n",
    "\n",
    "# Ejecutar el pipeline mejorado\n",
    "main_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e80d111-9b64-4908-8318-2eea9145426e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo características HOG...\n",
      "Normalizando características...\n",
      "Realizando búsqueda de hiperparámetros con GridSearchCV...\n",
      "Mejores parámetros encontrados: {'C': 1, 'gamma': 'scale'}\n",
      "Clasificación:\n",
      "\n",
      "Clase gatos:\n",
      "  precisión: 0.99\n",
      "\n",
      "Clase perros:\n",
      "  precisión: 0.81\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# 1. Preprocesamiento\n",
    "def preprocesar_imagen(ruta_imagen):\n",
    "    imagen = cv2.imread(ruta_imagen, cv2.IMREAD_GRAYSCALE)\n",
    "    if imagen is None:  # Manejar imágenes no cargadas\n",
    "        print(f\"Error al cargar la imagen: {ruta_imagen}\")\n",
    "        return None\n",
    "    imagen = cv2.resize(imagen, (128, 128))  # Redimensionar a 128x128\n",
    "    imagen = cv2.equalizeHist(imagen)        # Ecualización del histograma\n",
    "    return imagen\n",
    "\n",
    "# 2. Extracción de características HOG\n",
    "def extraer_caracteristicas_hog(imagen):\n",
    "    fd = hog(\n",
    "        imagen,\n",
    "        orientations=9,  # Aumentamos el número de orientaciones\n",
    "        pixels_per_cell=(8, 8),  # Reducimos el tamaño de las celdas para capturar más detalles\n",
    "        cells_per_block=(2, 2),  # Aumentamos el tamaño del bloque para mejorar la normalización\n",
    "        visualize=False  # Solo devuelve el descriptor\n",
    "    )\n",
    "    return fd\n",
    "\n",
    "# 3. Creación del dataset\n",
    "def crear_dataset(ruta_imagenes, extractor_caracteristicas):\n",
    "    \"\"\"\n",
    "    Crea un dataset de características y etiquetas a partir de un directorio.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for etiqueta, nombre_clase in enumerate(['cat', 'dog']):\n",
    "        directorio_clase = os.path.join(ruta_imagenes, nombre_clase)\n",
    "        for nombre_archivo in os.listdir(directorio_clase):\n",
    "            ruta_imagen = os.path.join(directorio_clase, nombre_archivo)\n",
    "            imagen = preprocesar_imagen(ruta_imagen)\n",
    "            if imagen is None:  # Omitir imágenes no válidas\n",
    "                continue\n",
    "            caracteristicas = extractor_caracteristicas(imagen)\n",
    "            X.append(caracteristicas)\n",
    "            y.append(etiqueta)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 4. Clasificación\n",
    "def clasificacion(y_real, y_predicha):\n",
    "    nombres_clases = {0: \"gatos\", 1: \"perros\"}\n",
    "\n",
    "    # Inicializar un diccionario para almacenar las métricas por clase\n",
    "    reporte = {}\n",
    "    for clase in [0, 1]:\n",
    "        # Calcular las métricas por clase\n",
    "        precision = precision_score(y_real, y_predicha, labels=[clase], average=\"micro\")\n",
    "        reporte[nombres_clases[clase]] = {\n",
    "            'precisión': precision\n",
    "        }\n",
    "    # Imprimir clasificación\n",
    "    print(\"Clasificación:\")\n",
    "    for nombre_clase, metricas in reporte.items():\n",
    "        print(f\"\\nClase {nombre_clase}:\")\n",
    "        for metrica, valor in metricas.items():\n",
    "            print(f\"  {metrica}: {valor:.2f}\")\n",
    "\n",
    "# Pipeline actualizado con búsqueda de hiperparámetros (GridSearchCV)\n",
    "def pipeline_principal():\n",
    "    ruta_train = 'cat_dog_500/train'\n",
    "    ruta_test = 'cat_dog_500/test'\n",
    "\n",
    "    print(\"Extrayendo características HOG...\")\n",
    "    X_train, y_train = crear_dataset(ruta_train, extraer_caracteristicas_hog)\n",
    "    X_test, y_test = crear_dataset(ruta_test, extraer_caracteristicas_hog)\n",
    "\n",
    "    print(\"Normalizando características...\")\n",
    "    # Normalizamos las características\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Realizando búsqueda de hiperparámetros con GridSearchCV...\")\n",
    "    parametros = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}\n",
    "    clf = SVC(kernel='rbf')\n",
    "    busqueda_grid = GridSearchCV(clf, parametros, cv=5, scoring='accuracy')\n",
    "    busqueda_grid.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluando el modelo con los mejores parámetros\n",
    "    mejor_modelo = busqueda_grid.best_estimator_\n",
    "    y_predicha = mejor_modelo.predict(X_test)\n",
    "    clasificacion(y_test, y_predicha)\n",
    "\n",
    "# Ejecutar el pipeline mejorado\n",
    "pipeline_principal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de689539-90d1-40b7-aa04-1c12da763e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 499)\n",
      "0\n",
      "(375, 499)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#Pruebas harrys\n",
    "import cv2\n",
    "import numpy as np\n",
    "imagen = cv2.imread(\"cat_dog_100/test/cat/cat.10003.jpg\", 0)\n",
    "print(imagen.shape)\n",
    "imagen = cv2.cornerHarris(imagen,2,3,0.04) \n",
    "imagen = np.uint8(imagen)\n",
    "print(np.max(imagen))\n",
    "print(imagen.shape)\n",
    "print(imagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a876f780-a5c5-49be-8728-45a7833e6d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99,)\n",
      "(24,)\n",
      "(24, 2)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def harris_corner_detection(image_path):\n",
    "    # Cargar la imagen en escala de grises\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convertir a tipo float32\n",
    "    gray = np.float32(gray)\n",
    "\n",
    "    # Aplicar el detector de esquinas de Harris\n",
    "    dst = cv2.cornerHarris(src=gray, blockSize=5, ksize=5, k=0.04)\n",
    "    posiciones, valores = maximos_locales(dst)\n",
    "    print(valores.shape)\n",
    "    posiciones = posiciones[valores > 0.01 * np.max(dst)]\n",
    "    valores = valores[valores > 0.01 * np.max(dst)]\n",
    "    print(valores.shape)\n",
    "    print(posiciones.shape)\n",
    "# Ruta de la imagen (reemplázala con la ruta a tu imagen)\n",
    "image_path = \"cat_dog_100/test/cat/cat.10003.jpg\"\n",
    "harris_corner_detection(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b80382be-ebc2-481e-9172-650989d313d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import maximum_filter\n",
    "def maximos_locales(matriz):\n",
    "    # Aplicar un filtro de máxima vecindad 3x3\n",
    "    vecinos_maximos = maximum_filter(matriz, size=3, mode='constant', cval=0)\n",
    "    \n",
    "    # Identificar dónde los valores coinciden con el filtro (máximos locales)\n",
    "    maximos = (matriz == vecinos_maximos)\n",
    "    \n",
    "    # Ignorar valores cero si es necesario\n",
    "    maximos = maximos & (matriz > 0)\n",
    "    \n",
    "    # Obtener las posiciones de los máximos locales\n",
    "    posiciones = np.argwhere(maximos)\n",
    "    \n",
    "    return posiciones, matriz[maximos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dd510897-f7da-4b9e-9504-e926fc42ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Función para dividir la imagen en bloques de 8x8 y calcular la media de cada bloque\n",
    "def dividir_y_media(imagen, bloque_size):\n",
    "    # Obtener las dimensiones de la imagen\n",
    "    alto, ancho = imagen.shape\n",
    "    \n",
    "    # Asegurarse de que las dimensiones de la imagen son divisibles por el tamaño del bloque\n",
    "    assert alto % bloque_size == 0 and ancho % bloque_size == 0, \"Las dimensiones de la imagen deben ser divisibles por el tamaño del bloque\"\n",
    "    \n",
    "    # Dividir la imagen en bloques\n",
    "    bloque_matriz = np.zeros((alto // bloque_size, ancho // bloque_size))  # Matriz de medias\n",
    "\n",
    "    for i in range(0, alto, bloque_size):\n",
    "        for j in range(0, ancho, bloque_size):\n",
    "            # Extraer el bloque 8x8\n",
    "            bloque = imagen[i:i + bloque_size, j:j + bloque_size]\n",
    "            # Calcular la media del bloque\n",
    "            bloque_matriz[i // bloque_size, j // bloque_size] = np.mean(bloque)\n",
    "    \n",
    "    return bloque_matriz\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
