{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8eeb72a-d367-4376-9e3c-5c092fd7207e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rembg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrembg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rembg'"
     ]
    }
   ],
   "source": [
    "from rembg import remove\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Cargar imagen y eliminar fondo\n",
    "with open('cat_dog_100/train/dog/dog.10095.jpg', 'rb') as f:\n",
    "    input_image = f.read()\n",
    "\n",
    "output_image = remove(input_image)\n",
    "\n",
    "# Guardar el resultado sin fondo\n",
    "with open('perro_sin_fondo.png', 'wb') as f:\n",
    "    f.write(output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2723050d-c0da-45a5-bb6f-31c0c51f9a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[191 186 156 ... 124 149 166]\n",
      " [175 169 153 ... 135 146 154]\n",
      " [156 148 154 ... 134 130 129]\n",
      " ...\n",
      " [188 171 155 ... 187 169 132]\n",
      " [116 133 156 ... 192 186 157]\n",
      " [116 138 161 ... 212 223 215]]\n",
      "[[255 255 156 ... 124 149 255]\n",
      " [255 255 153 ... 135 146 154]\n",
      " [156 148 154 ... 134 130 129]\n",
      " ...\n",
      " [255 255 155 ... 255 255 132]\n",
      " [116 133 156 ... 255 255 157]\n",
      " [116 138 255 ... 255 255 255]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "imagen = cv2.imread('cat_dog_100/train/dog/dog.10089.jpg', 0)\n",
    "h, bins = np.histogram (imagen, bins = 256, range = [0, 256])\n",
    "q = np.median(imagen)\n",
    "\n",
    "print(imagen)\n",
    "segmentada = imagen\n",
    "segmentada[imagen>q] = 255\n",
    "print(segmentada)\n",
    "\n",
    "          \n",
    "cv2.imshow('dst',segmentada)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6af7eba7-73a1-4f74-b405-8e3ce358a476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo características HOG...\n",
      "Entrenando modelo SVM...\n",
      "Evaluando modelo...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.59      0.67       100\n",
      "           1       0.67      0.83      0.74       100\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.72      0.71      0.71       200\n",
      "weighted avg       0.72      0.71      0.71       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Preprocesamiento\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:  # Manejar imágenes no cargadas\n",
    "        print(f\"Error al cargar la imagen: {image_path}\")\n",
    "        return None\n",
    "    image = cv2.resize(image, (128, 128))  # Redimensionar a 128x128\n",
    "    image = cv2.equalizeHist(image)        # Ecualización del histograma\n",
    "    return image\n",
    "\n",
    "# 2. Extracción de características HOG\n",
    "def extract_hog_features(image):\n",
    "\n",
    "    fd = hog(\n",
    "        image,\n",
    "        orientations=8,\n",
    "        pixels_per_cell=(16, 16),\n",
    "        cells_per_block=(1, 1),\n",
    "        visualize=False  # Solo devuelve el descriptor\n",
    "    )\n",
    "    return fd\n",
    "\n",
    "# 3. Creación del dataset\n",
    "def create_dataset(image_dir, feature_extractor):\n",
    "    \"\"\"\n",
    "    Crea un dataset de características y etiquetas a partir de un directorio.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for label, class_name in enumerate(['cat', 'dog']):\n",
    "        class_dir = os.path.join(image_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, file_name)\n",
    "            image = preprocess_image(image_path)\n",
    "            if image is None:  # Omitir imágenes no válidas\n",
    "                continue\n",
    "            features = feature_extractor(image)\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def main_pipeline():\n",
    "    # Definir rutas de datos\n",
    "    train_dir = 'cat_dog_500/train'\n",
    "    test_dir = 'cat_dog_500/test'\n",
    "\n",
    "    # Crear datasets usando HOG\n",
    "    print(\"Extrayendo características HOG...\")\n",
    "    X_train, y_train = create_dataset(train_dir, extract_hog_features)\n",
    "    X_test, y_test = create_dataset(test_dir, extract_hog_features)\n",
    "\n",
    "    # Normalización (opcional si las características no están en el mismo rango)\n",
    "    X_train = (X_train - np.min(X_train)) / (np.max(X_train) - np.min(X_train))\n",
    "    X_test = (X_test - np.min(X_test)) / (np.max(X_test) - np.min(X_test))\n",
    "\n",
    "    # Entrenamiento del modelo\n",
    "    print(\"Entrenando modelo SVM...\")\n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluación del modelo\n",
    "    print(\"Evaluando modelo...\")\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Ejecutar el pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    main_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0bf47d1-6e58-402d-8a49-1a2f0dbc78d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo características HOG...\n",
      "Entrenando modelo SVM...\n",
      "Evaluando modelo...\n",
      "Reporte de Clasificación :\n",
      "\n",
      "Clase gatos:\n",
      "  precision: 0.78\n",
      "\n",
      "Clase perros:\n",
      "  precision: 0.67\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Preprocesamiento\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:  # Manejar imágenes no cargadas\n",
    "        print(f\"Error al cargar la imagen: {image_path}\")\n",
    "        return None\n",
    "    image = cv2.resize(image, (128, 128))  # Redimensionar a 128x128\n",
    "    image = cv2.equalizeHist(image)        # Ecualización del histograma\n",
    "    return image\n",
    "\n",
    "# 2. Extracción de características HOG\n",
    "def extract_hog_features(image):\n",
    "\n",
    "    fd = hog(\n",
    "        image,\n",
    "        orientations=8,\n",
    "        pixels_per_cell=(16, 16),\n",
    "        cells_per_block=(1, 1),\n",
    "        visualize=False  # Solo devuelve el descriptor\n",
    "    )\n",
    "    return fd\n",
    "\n",
    "# 3. Creación del dataset\n",
    "def create_dataset(image_dir, feature_extractor):\n",
    "    \"\"\"\n",
    "    Crea un dataset de características y etiquetas a partir de un directorio.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for label, class_name in enumerate(['cat', 'dog']):\n",
    "        class_dir = os.path.join(image_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, file_name)\n",
    "            image = preprocess_image(image_path)\n",
    "            if image is None:  # Omitir imágenes no válidas\n",
    "                continue\n",
    "            features = feature_extractor(image)\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def clasificacion(y_true, y_pred):\n",
    "    # Definir nombres de clases\n",
    "    class_names = {0: \"gatos\", 1: \"perros\"}\n",
    "\n",
    "    # Inicializar un diccionario para almacenar las métricas por clase\n",
    "    report = {}\n",
    "    for cls in [0, 1]:\n",
    "        # Calcular las métricas por clase\n",
    "        precision = precision_score(y_true, y_pred, labels=[cls], average=\"micro\")\n",
    "        report[class_names[cls]] = {\n",
    "            'precision': precision\n",
    "        }\n",
    "    # Imprimir el reporte de clasificación manual\n",
    "    print(\"Reporte de Clasificación :\")\n",
    "    for class_name, metrics in report.items():\n",
    "        print(f\"\\nClase {class_name}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"  {metric}: {value:.2f}\")\n",
    "\n",
    "# Pipeline actualizado\n",
    "def main_pipeline():\n",
    "    train_dir = 'cat_dog_500/train'\n",
    "    test_dir = 'cat_dog_500/test'\n",
    "\n",
    "    print(\"Extrayendo características HOG...\")\n",
    "    X_train, y_train = create_dataset(train_dir, extract_hog_features)\n",
    "    X_test, y_test = create_dataset(test_dir, extract_hog_features)\n",
    "\n",
    "    print(\"Entrenando modelo SVM...\")\n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Evaluando modelo...\")\n",
    "    y_pred = clf.predict(X_test)\n",
    "    clasificacion(y_test, y_pred)\n",
    "\n",
    "\n",
    "main_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd66192b-0904-4d08-a56b-e86dde710166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo características HOG...\n",
      "Normalizando características...\n",
      "Realizando búsqueda de hiperparámetros con GridSearchCV...\n",
      "Mejores parámetros encontrados: {'C': 1, 'gamma': 'scale'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       gatos       0.99      0.77      0.87       100\n",
      "      perros       0.81      0.99      0.89       100\n",
      "\n",
      "    accuracy                           0.88       200\n",
      "   macro avg       0.90      0.88      0.88       200\n",
      "weighted avg       0.90      0.88      0.88       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Preprocesamiento\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:  # Manejar imágenes no cargadas\n",
    "        print(f\"Error al cargar la imagen: {image_path}\")\n",
    "        return None\n",
    "    image = cv2.resize(image, (128, 128))  # Redimensionar a 128x128\n",
    "    image = cv2.equalizeHist(image)        # Ecualización del histograma\n",
    "    return image\n",
    "\n",
    "# 2. Extracción de características HOG\n",
    "def extract_hog_features(image):\n",
    "    fd = hog(\n",
    "        image,\n",
    "        orientations=9,  # Aumentamos el número de orientaciones\n",
    "        pixels_per_cell=(8, 8),  # Reducimos el tamaño de las celdas para capturar más detalles\n",
    "        cells_per_block=(2, 2),  # Aumentamos el tamaño del bloque para mejorar la normalización\n",
    "        visualize=False  # Solo devuelve el descriptor\n",
    "    )\n",
    "    return fd\n",
    "\n",
    "# 3. Creación del dataset\n",
    "def create_dataset(image_dir, feature_extractor):\n",
    "    \"\"\"\n",
    "    Crea un dataset de características y etiquetas a partir de un directorio.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for label, class_name in enumerate(['cat', 'dog']):\n",
    "        class_dir = os.path.join(image_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, file_name)\n",
    "            image = preprocess_image(image_path)\n",
    "            if image is None:  # Omitir imágenes no válidas\n",
    "                continue\n",
    "            features = feature_extractor(image)\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 4. Clasificación\n",
    "def clasificacion(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"gatos\", \"perros\"]))\n",
    "\n",
    "# Pipeline actualizado con búsqueda de hiperparámetros (GridSearchCV)\n",
    "def main_pipeline():\n",
    "    train_dir = 'cat_dog_500/train'\n",
    "    test_dir = 'cat_dog_500/test'\n",
    "\n",
    "    print(\"Extrayendo características HOG...\")\n",
    "    X_train, y_train = create_dataset(train_dir, extract_hog_features)\n",
    "    X_test, y_test = create_dataset(test_dir, extract_hog_features)\n",
    "\n",
    "    print(\"Normalizando características...\")\n",
    "    # Normalizamos las características\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Realizando búsqueda de hiperparámetros con GridSearchCV...\")\n",
    "    parameters = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}\n",
    "    clf = SVC(kernel='rbf')\n",
    "    grid_search = GridSearchCV(clf, parameters, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Obtener los mejores parámetros\n",
    "    print(\"Mejores parámetros encontrados:\", grid_search.best_params_)\n",
    "\n",
    "    # Evaluando el modelo con los mejores parámetros\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    clasificacion(y_test, y_pred)\n",
    "\n",
    "# Ejecutar el pipeline mejorado\n",
    "main_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e80d111-9b64-4908-8318-2eea9145426e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo características HOG...\n",
      "Normalizando características...\n",
      "Realizando búsqueda de hiperparámetros con GridSearchCV...\n",
      "Mejores parámetros encontrados: {'C': 1, 'gamma': 'scale'}\n",
      "Clasificación:\n",
      "\n",
      "Clase gatos:\n",
      "  precisión: 0.99\n",
      "\n",
      "Clase perros:\n",
      "  precisión: 0.81\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# 1. Preprocesamiento\n",
    "def preprocesar_imagen(ruta_imagen):\n",
    "    imagen = cv2.imread(ruta_imagen, cv2.IMREAD_GRAYSCALE)\n",
    "    if imagen is None:  # Manejar imágenes no cargadas\n",
    "        print(f\"Error al cargar la imagen: {ruta_imagen}\")\n",
    "        return None\n",
    "    imagen = cv2.resize(imagen, (128, 128))  # Redimensionar a 128x128\n",
    "    imagen = cv2.equalizeHist(imagen)        # Ecualización del histograma\n",
    "    return imagen\n",
    "\n",
    "# 2. Extracción de características HOG\n",
    "def extraer_caracteristicas_hog(imagen):\n",
    "    fd = hog(\n",
    "        imagen,\n",
    "        orientations=9,  # Aumentamos el número de orientaciones\n",
    "        pixels_per_cell=(8, 8),  # Reducimos el tamaño de las celdas para capturar más detalles\n",
    "        cells_per_block=(2, 2),  # Aumentamos el tamaño del bloque para mejorar la normalización\n",
    "        visualize=False  # Solo devuelve el descriptor\n",
    "    )\n",
    "    return fd\n",
    "\n",
    "# 3. Creación del dataset\n",
    "def crear_dataset(ruta_imagenes, extractor_caracteristicas):\n",
    "    \"\"\"\n",
    "    Crea un dataset de características y etiquetas a partir de un directorio.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for etiqueta, nombre_clase in enumerate(['cat', 'dog']):\n",
    "        directorio_clase = os.path.join(ruta_imagenes, nombre_clase)\n",
    "        for nombre_archivo in os.listdir(directorio_clase):\n",
    "            ruta_imagen = os.path.join(directorio_clase, nombre_archivo)\n",
    "            imagen = preprocesar_imagen(ruta_imagen)\n",
    "            if imagen is None:  # Omitir imágenes no válidas\n",
    "                continue\n",
    "            caracteristicas = extractor_caracteristicas(imagen)\n",
    "            X.append(caracteristicas)\n",
    "            y.append(etiqueta)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 4. Clasificación\n",
    "def clasificacion(y_real, y_predicha):\n",
    "    nombres_clases = {0: \"gatos\", 1: \"perros\"}\n",
    "\n",
    "    # Inicializar un diccionario para almacenar las métricas por clase\n",
    "    reporte = {}\n",
    "    for clase in [0, 1]:\n",
    "        # Calcular las métricas por clase\n",
    "        precision = precision_score(y_real, y_predicha, labels=[clase], average=\"micro\")\n",
    "        reporte[nombres_clases[clase]] = {\n",
    "            'precisión': precision\n",
    "        }\n",
    "    # Imprimir clasificación\n",
    "    print(\"Clasificación:\")\n",
    "    for nombre_clase, metricas in reporte.items():\n",
    "        print(f\"\\nClase {nombre_clase}:\")\n",
    "        for metrica, valor in metricas.items():\n",
    "            print(f\"  {metrica}: {valor:.2f}\")\n",
    "\n",
    "# Pipeline actualizado con búsqueda de hiperparámetros (GridSearchCV)\n",
    "def pipeline_principal():\n",
    "    ruta_train = 'cat_dog_500/train'\n",
    "    ruta_test = 'cat_dog_500/test'\n",
    "\n",
    "    print(\"Extrayendo características HOG...\")\n",
    "    X_train, y_train = crear_dataset(ruta_train, extraer_caracteristicas_hog)\n",
    "    X_test, y_test = crear_dataset(ruta_test, extraer_caracteristicas_hog)\n",
    "\n",
    "    print(\"Normalizando características...\")\n",
    "    # Normalizamos las características\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Realizando búsqueda de hiperparámetros con GridSearchCV...\")\n",
    "    parametros = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}\n",
    "    clf = SVC(kernel='rbf')\n",
    "    busqueda_grid = GridSearchCV(clf, parametros, cv=5, scoring='accuracy')\n",
    "    busqueda_grid.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluando el modelo con los mejores parámetros\n",
    "    mejor_modelo = busqueda_grid.best_estimator_\n",
    "    y_predicha = mejor_modelo.predict(X_test)\n",
    "    clasificacion(y_test, y_predicha)\n",
    "\n",
    "# Ejecutar el pipeline mejorado\n",
    "pipeline_principal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de689539-90d1-40b7-aa04-1c12da763e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(375, 499)\n",
      "0\n",
      "(375, 499)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#Pruebas harrys\n",
    "import cv2\n",
    "import numpy as np\n",
    "imagen = cv2.imread(\"cat_dog_100/test/cat/cat.10003.jpg\", 0)\n",
    "print(imagen.shape)\n",
    "imagen = cv2.cornerHarris(imagen,2,3,0.04) \n",
    "imagen = np.uint8(imagen)\n",
    "print(np.max(imagen))\n",
    "print(imagen.shape)\n",
    "print(imagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51550f74-6489-43e1-8ec3-15413bca439f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261,)\n",
      "(46,)\n",
      "(46, 2)\n",
      "[[ 4.61059928e-01  7.91693181e-02  4.90594053e+00  4.65978719e+05\n",
      "  -3.93766031e+05  1.13417800e+06  1.20932977e+05  2.35521031e+05]\n",
      " [ 1.32167000e+05  5.58460156e+03 -3.75312477e-01  2.30647900e+06\n",
      "  -2.02303812e+05  2.94167500e+05  1.02522575e+06  1.20855000e+06]\n",
      " [ 1.67733960e+03  5.32492041e+03 -5.25376289e+04  1.87280625e+06\n",
      "   3.03566650e+06  7.88391438e+05  7.84398375e+05  9.59538062e+05]\n",
      " [ 9.64690234e+03  1.02857598e+04 -1.15484650e+06  8.52559000e+05\n",
      "   1.09412040e+07  1.42764625e+05  2.53263062e+05  1.56362469e+05]\n",
      " [ 1.30747807e+00  2.63560229e+03 -4.13080625e+05 -1.77883225e+06\n",
      "   7.76082812e+04  4.27502109e+04 -1.57965391e+05 -5.07622852e+04]\n",
      " [ 8.08658695e+00 -1.12836599e+00 -1.08131288e+06 -5.66537438e+05\n",
      "  -1.80576147e+03  1.69197969e+04 -1.90720012e+06 -4.36385219e+05]\n",
      " [ 1.67506289e+04 -1.39084863e+04  1.09501270e+04 -5.76452562e+05\n",
      "   1.89528477e+04  1.08801230e+04 -1.51936547e+05 -4.58438633e+04]\n",
      " [-4.30090312e+04 -1.04779414e+05 -1.32314531e+05 -1.08839094e+05\n",
      "   1.14036670e+04 -4.40295250e+05  7.54170375e+05  4.73595312e+04]]\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def harris_corner_detection(image_path):\n",
    "    # Cargar la imagen en escala de grises\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convertir a tipo float32\n",
    "    gray = np.float32(gray)\n",
    "\n",
    "    # Aplicar el detector de esquinas de Harris\n",
    "    dst = cv2.cornerHarris(src=gray, blockSize=2, ksize=3, k=0.04)\n",
    "    posiciones, valores = maximos_locales(dst)\n",
    "    print(valores.shape)\n",
    "    posiciones = posiciones[valores > 0.01 * np.max(dst)]\n",
    "    valores = valores[valores > 0.01 * np.max(dst)]\n",
    "    print(valores.shape)\n",
    "    print(posiciones.shape)\n",
    "    reducir_mat2(8, dst)\n",
    "    print(len(dst))\n",
    "# Ruta de la imagen (reemplázala con la ruta a tu imagen)\n",
    "image_path = \"cat_dog_100/test/cat/cat.10003.jpg\"\n",
    "harris_corner_detection(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80382be-ebc2-481e-9172-650989d313d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import maximum_filter\n",
    "def maximos_locales(matriz):\n",
    "    # Aplicar un filtro de máxima vecindad 3x3\n",
    "    vecinos_maximos = maximum_filter(matriz, size=3, mode='constant', cval=0)\n",
    "    \n",
    "    # Identificar dónde los valores coinciden con el filtro (máximos locales)\n",
    "    maximos = (matriz == vecinos_maximos)\n",
    "    \n",
    "    # Ignorar valores cero si es necesario\n",
    "    maximos = maximos & (matriz > 0)\n",
    "    \n",
    "    # Obtener las posiciones de los máximos locales\n",
    "    posiciones = np.argwhere(maximos)\n",
    "    \n",
    "    return posiciones, matriz[maximos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd510897-f7da-4b9e-9504-e926fc42ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducir_mat(tamaño, posiciones, valores):\n",
    "    filas = tamaño\n",
    "    columnas = tamaño\n",
    "    print(posiciones[0])\n",
    "    mat = np.array([[tamaño], [tamaño]])\n",
    "    fila_ant =  0\n",
    "    col_ant = 0\n",
    "    for i in range(len(posiciones)):\n",
    "        vect = np.array([])\n",
    "        fila = posiciones[i][0] % tamaño\n",
    "        columna = posiciones[i][1] % tamaño\n",
    "        if (fila == fila_ant and columna == col_ant):\n",
    "            vect.append(valores[fila, columna])\n",
    "        else:\n",
    "            if(np.isnan(vect)):\n",
    "                vect = 0\n",
    "            mat[fila_ant][col_ant] = np.median(vect)\n",
    "            vect.append(valores[fila, columna])\n",
    "            \n",
    "        # Asignamos el valor a la matriz en las posiciones calculadas\n",
    "        fila_ant = fila\n",
    "        col_ant = columna\n",
    "    print(mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1040546b-67ff-4494-953a-74c55233d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reducir_mat(tamaño, posiciones, valores):\n",
    "    # Crear una matriz de ceros (sin usar acumuladores de listas)\n",
    "    mat = np.zeros((tamaño, tamaño), dtype=float)\n",
    "\n",
    "    # Crear un diccionario para agrupar los valores por posiciones únicas\n",
    "    acumulador = {}\n",
    "\n",
    "    # Agrupar los valores según fila % tamaño y columna % tamaño\n",
    "    for i in range(len(posiciones)):\n",
    "        fila = posiciones[i][0] % tamaño\n",
    "        columna = posiciones[i][1] % tamaño\n",
    "        if (fila, columna) not in acumulador:\n",
    "            acumulador[(fila, columna)] = []\n",
    "        acumulador[(fila, columna)].append(valores[i])\n",
    "\n",
    "    # Calcular la mediana para cada celda con valores\n",
    "    for (fila, columna), vals in acumulador.items():\n",
    "        mat[fila, columna] = np.median(vals)\n",
    "\n",
    "    # Mostrar la matriz resultante\n",
    "    print(\"Matriz resultante:\")\n",
    "    print(mat)\n",
    "    return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b22567d7-7653-4189-95d6-f54bb7aa831b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10.079602241516113, 76.81034851074219)\n",
      "0.02941621094942093\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'response'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(keypoints[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpt)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(keypoints[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mresponse)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmax(\u001b[43mkeypoints\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(descriptors)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'response'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image_path = \"cat_dog_100/test/cat/cat.10003.jpg\"\n",
    "imagen = cv2.imread(image_path, 0)\n",
    "imagen = np.resize(imagen, (128, 128))\n",
    "sift = cv2.SIFT_create(contrastThreshold=0.04, edgeThreshold=15)\n",
    "keypoints, descriptors = sift.detectAndCompute(imagen, None)\n",
    "print(keypoints[0].pt)\n",
    "print(keypoints[0].response)\n",
    "print(np.max(keypoints.response))\n",
    "print(descriptors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "057ef8b5-f91e-40c4-9de7-098fd0010910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reducir_mat2(tamaño, matriz):\n",
    "    # Crear una matriz de ceros (sin usar acumuladores de listas)\n",
    "    mat = np.zeros((tamaño, tamaño), dtype=float)\n",
    "    filas_sub, columnas_sub = tamaño, tamaño\n",
    "\n",
    "    # Crear un diccionario para agrupar los valores por posiciones únicas\n",
    "    acumulador = {}\n",
    "\n",
    "    # Agrupar los valores según fila % tamaño y columna % tamaño\n",
    "    submatrices = [matriz[i:i+filas_sub, j:j+columnas_sub]\n",
    "               for i in range(0, matriz.shape[0], filas_sub)\n",
    "               for j in range(0, matriz.shape[1], columnas_sub)]\n",
    "\n",
    "    # Calcular la mediana para cada celda con valores\n",
    "    medias = [np.mean(submatriz) for submatriz in submatrices]\n",
    "    \n",
    "    # Crear una nueva matriz de medias (2x2)\n",
    "    matriz_medias = np.array(medias).reshape(tamaño, tamaño)\n",
    "    print(matriz_medias)\n",
    "    return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "972ab06a-b122-4055-a367-d0548f57340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submatriz 1:\n",
      "[[1 2]\n",
      " [5 6]]\n",
      "\n",
      "Submatriz 2:\n",
      "[[3 4]\n",
      " [7 8]]\n",
      "\n",
      "Submatriz 3:\n",
      "[[9 0]\n",
      " [3 4]]\n",
      "\n",
      "Submatriz 4:\n",
      "[[1 2]\n",
      " [5 6]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Matriz original\n",
    "matriz = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 0, 1, 2],\n",
    "    [3, 4, 5, 6]\n",
    "])\n",
    "\n",
    "# Tamaño de las submatrices\n",
    "filas_sub, columnas_sub = 2, 2\n",
    "\n",
    "# Dividir la matriz en submatrices\n",
    "submatrices = [matriz[i:i+filas_sub, j:j+columnas_sub]\n",
    "               for i in range(0, matriz.shape[0], filas_sub)\n",
    "               for j in range(0, matriz.shape[1], columnas_sub)]\n",
    "\n",
    "# Imprimir las submatrices\n",
    "for idx, submatriz in enumerate(submatrices, start=1):\n",
    "    print(f\"Submatriz {idx}:\\n{submatriz}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
